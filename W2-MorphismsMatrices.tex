\documentclass{exam}
\usepackage[utf8]{inputenc}

\usepackage{mynotes}

\title{Honours Algebra - Week 2 - Linear Mappings as Matrices}
\author{Antonio León Villares}
\date{January 2022}

\begin{document}

\maketitle

\tableofcontents

\pagebreak

\textit{Based on the notes by Iain Gordon, Sections 1.7 - 1.8 \& 2.1 - 2.2}

\section{Linear Mappings}

\subsection{The Morphisms}

\begin{itemize}
    \item \textbf{What is a homomorphism?}
    \begin{itemize}
        \item let $V,W$ be vector spaces
        \item let $\vec{v}_1, \vec{v}_2 \in V$ and $\lambda \in \F$
        \item a \textbf{homomorphism of $\F$ - vector spaces} is  mapping of the form:
        \[
        f : V \to W
        \]
        such that:
        \[
        f(\vec{v}_1 + \vec{v}_2) = f(\vec{v}_2) + f(\vec{v}_2)
        \]
        \[
        f(\lambda \vec{v}_1) = \lambda f(\vec{v}_1)
        \]
        \item this is also known as a \textbf{linear mapping} (or a F-linear mapping)
    \end{itemize}
    \item \textbf{What is an isomorphism?}
    \begin{itemize}
        \item a \textbf{bijective homomorphism}
    \end{itemize}
    \item \textbf{What are isomorphic vector spaces?}
    \begin{itemize}
        \item vector spaces for which an \textbf{isomorphism} exists between the two
    \end{itemize}
    \item \textbf{What is an endomorphism?}
    \begin{itemize}
        \item a \textbf{homomorphism} from a vector space to \textbf{itself}
    \end{itemize}
    \item \textbf{What is an automorphism?}
    \begin{itemize}
        \item an \textbf{isomorphism} from a vector space to \textbf{itself}
    \end{itemize}
    \item \textbf{What is a fixed point of a mapping?}
    \begin{itemize}
        \item given a mapping $f : X \to X$, a \textbf{fixed point} of $f$ is a point $x \in X$ sent to itself under $f$
        \item the set of all fixed points is:
        \[
        X^f = \{x | x \in X, f(x) = x\}
        \]
    \end{itemize}
\end{itemize}

\subsubsection{Examples}

\begin{itemize}
    \item projections are linear mappings:
    \[
    pr_i : (\lambda_1, \lambda_2, \ldots, \lambda_n) \to \lambda_i
    \]
    \item squaring ($\lambda \to \lambda^2$) is \textbf{not} linear (except for $\F = \mathbb{Z}_2$)
    \item \textbf{projection mappings}:
    \[
    (V \oplus W) \to W \qquad (V \oplus W) \to V
    \]
    are linear
    \item the \textbf{canonical injections}:
    \[
    v \to (v,0) \qquad w \to (0,w)
    \]
    are linear
    \item the bijective map defining a linear combination of basis elements:
    \[
    (\lambda_1, \lambda_2, \ldots, \lambda_n) \to \sum_{i = 1}^n \lambda_i\vec{v}_i
    \]
    is linear
    \item the \textbf{automorphisms} of a vector space $V$ form a subgroup of its permutation group (known as \textbf{general linear group} or \textbf{automorphism group} of $V$)
\end{itemize}

\subsubsection{Exercises (TODO)}

\begin{questions}

\question \textbf{Show that a composition of homomorphisms is a homomorphism.}

\bigskip

Define homomorphisms $f : V \to W$ and $f : U \to V$. Consider the composition $f \circ g : U \to W$.

\bigskip

Consider $\lambda \vec{u}_1 + \vec{u}_2$. Then:
\begin{align*}
    &(f \circ g)(\lambda \vec{u}_1 + \vec{u}_2) \\
    = & f(g(\lambda \vec{u}_1 + \vec{u}_2)) \\
    = & f(g(\lambda \vec{u}_1) + g(\vec{u}_2)) \\
    = & f(g(\lambda \vec{u}_1)) + f(g(\vec{u}_2)) \\
    = & f(\lambda g(\vec{u}_1)) + f(g(\vec{u}_2)) \\
    = & \lambda (f \circ g)(\vec{u}_1) + (f \circ g)(\vec{u}_2) \\
\end{align*}

\bigskip

\question \textbf{Show that if $f : V \to W$ is an isomorphism, then $f^{-1} : W \to V$ is also an isomorphism}

\bigskip

We know that since $f$ is an isomorphism, it is a bijection, so its inverse $f^{-1}$ exists. We want to show that:
\[
f^{-1}(\lambda\vec{w}_1 + \vec{w}_2) = \lambda f^{-1}(\vec{w}_1) + f^{-1}(\vec{w}_2)
\]
We know that:
\[
\lambda\vec{w}_1 + \vec{w}_2  = f(f^{-1}(\lambda\vec{w}_1 + \vec{w}_2))
\]
Moreover,
\[
f(\lambda f^{-1}(\vec{w}_1 ) + f^{-1}(\vec{w}_2)) = \lambda f( f^{-1}(\vec{w}_1)) + f(f^{-1}(\vec{w}_2)) = \lambda\vec{w}_1 + \vec{w}_2
\]
In other words:
\[
f(\lambda f^{-1}(\vec{w}_1 + \vec{w}_2)) = f(\lambda f^{-1}(\vec{w}_1 ) + f^{-1}(\vec{w}_2)) \ \implies \ f^{-1}(\lambda\vec{w}_1 + \vec{w}_2) = \lambda f^{-1}(\vec{w}_1 ) + f^{-1}(\vec{w}_2)
\]

\question \textbf{Show that the image of a vector subspace under a homomorphism is again a vector subspace. Moreover, show that the preimage of a vector susbspace under a homomorphism is a vector subspace.}

\question \textbf{Consider a vector space $V$, with the set of its endomorphisms $End(V)$. Show that $V^f \subseteq V$ is a vector subspace.}

\question \textbf{Show that, given vector spaces $V_1, V_2, \ldots, V_n, W$ and homomorphisms $f_i : V_i \to W$, we can define a new homomorphism:
\[
f : V_1 \oplus V_2 \oplus \ldots \oplus V_n \to W
\]
via:
\[
f(v_1, v_2, \ldots, v_n) = \sum_{i = 1}^n f_i(v_i)
\]
Given the above, we can define a bijection:
\[
Hom(V_1, W) \times \ldots \times Hom(V_1, W) \to Hom(V_1 \oplus \ldots \oplus V_n, W)
\]}

\question \textbf{Show that, given vector spaces $W_1, W_2, \ldots, W_n, V$ and homomorphisms $g_i : V \to W_i$, we can define a new homomorphism:
\[
g : V \to W_1 \oplus W_2 \oplus \ldots \oplus W_n
\]
via:
\[
g(v) = (g_1(v), \ldots, g_n(v))
\]
Given the above, we can define a bijection:
\[
Hom(V, W_1) \times \ldots \times Hom(V, W_n) \to Hom(V, W_1 \oplus \ldots \oplus W_n)
\]}

\question \textbf{Let $X = \mathbb{R}^2$ be a vector space over $\F = \mathbb{R}$. Determine the fixed point set of the following functions:}

\begin{parts}

\part $f(a,b) \to (a,b)$

For this, $X^f = X$.

\part $f(a,b) \to (b,a)$

For this, the fixed point set is defined by all points for which $a = b$, so:
\[
X^f = \{(a,a) | a \in \F\}
\]
Notice, in this case, $X^f$ is the diagonal line through the origin, and $f$ is a function which \textit{reflects} points in $X$ about $X^f$.

\part $f(a,b) \to (-b,a)$

Notice, if $a = -b$ and $b = a$, this implies that $a = -a = b = -b$. In particular, we must then have:
\[
X^f = {(0,0)}
\]
Notice, $X^f$ is the origin, and $f$ defines a 90º anticlockwise rotation.

\end{parts}

\question \textbf{How many vector subspaces are there in $\mathbb{R}^2$ that are sent to themselves under the reflection $(x,y) \to (x,-y)$? Which vector subspaces in $\mathbb{R}^3$ are sent to themselves by the reflection $(x,y,z) \to (x,y,-z)$?}

\end{questions}

\subsection{Complementary Subspaces}

\begin{itemize}
    \item \textbf{What is a complementary subspace?}
    \begin{itemize}
        \item consider a vector space $V$ with subspaces $V_1, V_2$
        \item $V_1$ and $V_2$ are \textbf{complementary subspaces} if we can define a bijection:
        \[
        f: V_1 \times V_2 \to V
        \]
        via:
        \[
        f(\vec{v}_1, \vec{v}_2) = \vec{v}_1 + \vec{v}_2
        \]
    \end{itemize}
\end{itemize}

\subsubsection{Examples}

\begin{itemize}
    \item consider $V = \mathbb{R}^2$ defined over $\F = \mathbb{R}$. Then the subspaces:
    \[
    V_1 = \langle 1, 0\rangle = \{( \mu, 0) | \mu \in \F\}
    \]
    \[
    V_2 = \langle b, c \rangle = \{(\lambda b, \lambda c) | \lambda \in \F\}
    \]
    are \textbf{complementary}. We can see if the addition mapping:
    \[
    (( \mu, 0), (\lambda b, \lambda c)) \to (\mu + \lambda b, \lambda c)
    \]
    Notice, if $c = 0$, this won't be bijective (since $(\mu + \lambda b, \lambda c)$ can be generated using many different combinations of $\mu, \lambda b$). Hence, $V_1$ and $V_2$ won't be complementary subspaces (in fact, we would have that $V_1 = V_2$ - they are the same line).
    
    \bigskip
    
    If $c \neq 0$, then for any point $(x,y)$, we can find unique $\lambda$ satisfying $\lambda c = y$, and unique $\mu$ such that $\mu + \lambda b = x$. In other words, $V_1$ and $V_2$ are complementary when $c \neq 0$ (in other words, when the lines aren't parallel).
    
\end{itemize}

\subsubsection{Exercises}

\begin{questions}

\question \textbf{Show that the bijection defining complementary subspaces is an isomorphism:
\[
f : V_1 \oplus V_2 \to V
\]
Here $V_1 \oplus V_2$ is an \textit{internal direct sum} (not to be confused with \textit{external direct sum}: the IDS refers to an operation on subspaces, whilst the EDS is more generally applicable to vector spaces).}

\end{questions}

\subsection{Theorem: Classification of Vector Spaces by their Dimension}
\label{t177}
\textbox{Let $n \in \mathbb{N}$. A vector space over a field $\F$ is isomorphic to $\F^n$ \textbf{if and only if} it has dimension $n$.
\\
In other words, for finite dimensional vector spaces, up to isomorphism, all that ``matters" is its dimension. [Theorem 1.7.7]}

\begin{proof}

Let $V$ be vector spaces over $\F$.

\begin{enumerate}
    \item ($\Longleftarrow$): say $V$ has dimension $n$. Then, it has a basis:
    \[
    E = \{\vec{a}_1, \ldots, \vec{a}_n\}
    \]
    We know that, for basis elements, the following is a bijective map (Theorem 1.5.11):
    \[
    f : \F^n \to V
    \]
    given by:
    \[
    (\alpha_1, \ldots, \alpha_n) \to \sum_{i = 1}^n \alpha_i\vec{a}_i
    \]
    Moreover, this is a linear map, so in particular, it defines an isomorphism, as required.
    \item ($\Longrightarrow$): assume there exists an isomorphism:
    \[
    f: \F^n \to V
    \]
    We know that $\F^n$ has a basis of $n$ elements:
    \[
    E = \{\vec{e}_1, \ldots, \vec{e}_n\}
    \]
    Notice, since $f$ is a bijection, it suffices to show that $f(E)$ is a basis for $V$, since then $V$ will have a basis of $n$ elements, as required. Hence, we need to show that $f(E)$ is:
    \begin{itemize}
        \item \textit{a generating set for $V$}: pick $\vec{v} \in V$. Since $f$ is a bijection, $\exists ! \vec{x} \in F^n$ such that $f(\vec{x}) = \vec{v}$. Moreover, we can write $\vec{x}$ in terms of the basis elements of $\F^n$:
        \[
        \vec{x} = \sum_{i = 1}^n \alpha_i\vec{e}_i
        \]
        Hence, using the linearity of the homomorphism:
        \[
        f(\vec{x}) = \vec{v} \ \implies \ \sum_{i = 1}^n \alpha_if(\vec{e}_i) = \vec{v}
        \]
        In other words, the set $f(E)$ is generating.
        \item \textit{linearly independent}: since $E$ is a basis, it is linearly independent, so:
        \[
        \sum_{i = 1}^n \alpha_i\vec{e}_i = \vec{0}
        \]
        only if $\alpha_i = 0, \forall i \in [1,n]$. But then, applying $f$ means that:
        \[
        \sum_{i = 1}^n \alpha_if(\vec{e}_i) = f(\vec{0}) = \vec{0}
        \]
        In other words, the elements of $f(E)$ are also linearly independent.
    \end{itemize}
    Hence, $f(E)$ is a basis for $V$. Moreover, it contains $n$ elements, so $dim V = n$, as required.
\end{enumerate}

\end{proof}

\subsection{Lemma: Linear Mappings and Bases}\label{l178}

\textbox{Define the set of all \textbf{homomorphisms} between vector spaces $V,W$ as:
\[
Hom_\F(V,W) \subseteq Maps(V,W)
\]
Let $B$ be a \textbf{basis} for $V$. We can define a \textbf{bijection}:
\[
Hom_\F(V,W) \to Maps(B,W)
\]
via:
\[
f \to f_B
\]
where $f_B$ is $f$, with its domain restricted to $B$.
\\
This means that any homomorphism can be defined by the values it takes at a basis. [Lemma 1.7.8]
}


\begin{proof}

Let $\Phi$ define the bijection.

\bigskip

Recall, $\Phi$ is injective if:
\[
\Phi(f) = \Phi(g) \ \implies \ f = g
\]
Let $f,g$ be linear mappings $f,g : V \to W$. If $\forall \vec{v} \in B$ we have:
\[
f(v) = g(v)
\]
then we must have that, $\forall \lambda_i \in \F, v_i \in B$:
\[
\sum_{i = 1}^n \lambda_i f(v_i) = \sum_{i = 1}^n \lambda_i g(v_i)
\]
Since $f,g$ are homomorphisms, then:
\[
f\left(\sum_{i = 1}^n \lambda_i v_i \right) = g\left(\sum_{i = 1}^n \lambda_i v_i \right)
\]
In other words, if $f$ and $g$ are equal for each element in the basis $B$, then they must be equal for any element in $V$. Hence, $\Phi$ must be injective.

\bigskip

For surjectivity, we need to ensure that for each element $g \in Maps(B,W)$, we have at least one other element $\bar{g} \in Hom_\F(V,W)$, such that:
\[
\Phi(g) = \bar{g}
\]
Indeed, take any $g : B \to W$. We can extend it to a homomorphism of the form $\bar{g} : V \to W$. Notice, any element $v \in V$ can be written as:
\[
v = \sum_{i = 1}^n \lambda_i v_i
\]
where each $v_i \in B$. In other words, the mapping:
\[
\bar{g}(v) = \sum_{i = 1}^n \lambda_i g(v_i)
\]
is clearly a homomorphism. In other words, we can map any element $\bar{g} \in Hom_\F(V,W)$ to $g \in Maps(B,W)$ using the above.

\end{proof}

\subsubsection{Exercises (TODO)}

\begin{questions}

\question \textbf{Let $V,W$ be vector spaces over a field $\F$. Show that $Hom_\F(V,W)$ is a vector subspace of the set of all
mappings $Maps(V,W)$. Moreover, show that its vector space structure is given similarly to the free vector space. Show that:
\[
dim Hom_\F(V,W) = (dim V)(dimW)
\]
where I am using the convention $0 \times \infty = 0$}

\question \textbf{Let $V$ be a finite dimensional vector space, and let $U$ be a proper vector subspace. Show that there
exists at least one (and in fact many different) vector subspace(s) of $V$ complementary to $U$. If you’re brave, try to do
this also for not necessarily finite dimensional vector spaces}

\end{questions}

\subsection{Proposition: Left and Right Inverses}

\textbox{\begin{enumerate}
    \item Every injective homomorphism:
    \[
    f: V \to W
    \]
    has a \textbf{left inverse} $g : W \to V$ such that:
    \[
    g \circ f = 1_V
    \]
    \item Every surjective homomorphism:
    \[
    f: V \to W
    \]
    has a \textbf{right inverse} $g : W \to V$ such that:
    \[
    f \circ g = 1_W
    \]
\end{enumerate}
[Proposition 1.7.9]}

\begin{proof}

\begin{itemize}
    \item Existence of Left Inverse For Injective Mappings
    \begin{itemize}
        \item we begin by noting that $f(V)$ is a subspace of $W$
        \item in particular, by Exercise 2 above, we can find a subspace $U$ of $W$ which is \textbf{complementary} to $f(V)$ 
        \begin{itemize}
            \item in exercise 2 we would require $f(V)$ to be a proper subspace
            \item if it isn't, then $f(V) = W$, so $f$ would be surjective, and so an isomorphism
            \item isomorphisms are bijective, and so, have a left inverse
        \end{itemize}
        \item since $U$ and $f(V)$ are complementary, we know that $\forall w \in W$, we have unique $u \in U, f(v) \in f(V)$ such that:
        \[
        w = u + f(v)
        \]
        \item moreover, by injectivity of $f$, $f(v)$ is uniquely produced by $v \in V$
        \item hence, we can define a mapping $g : W \to V$ such that:
        \[
        g(w) = v, \qquad w = u + f(v)
        \]
        (Apparently this mapping then shows that $g(f(v)) = v$, as required)
    \end{itemize}
    \item Existence of Right Inverse For Surjective Mappings
    \begin{itemize}
        \item we can pick a basis $B \subseteq W$
        \item using the fact that $f$ is surjective, we can define a mapping of sets:
        \[
        \bar{g} : B \to V
        \]
        such that:
        \[
        f(\bar{g}(b)) = b
        \]
        (we can think of $\bar{g}$ as mapping basis elements to enough elements of $V$ such that $f$ can send them back to the basis elements)
        \item by \eqref{l178}, we know that there exists a bijection between $Hom_\F(V,W)$ and $Maps(B,W)$. In particular, we can find $g \in Hom_\F(V,W)$, such that for $b \in B$, we have $g(b) = \bar{g}(b)$
        \item thus, $\forall b \in B$, we have:
        \[
        f(g(b)) = b
        \]
        \item notice, if $w \in W$, then we can write:
        \[
        w = \sum_{i = 1}^n \alpha_i b_i
        \]
        \item but then, using the fact that $f,g$ are homomorphisms:
        \begin{align*}
            f(g(w)) &= f\left(g\left(\sum_{i = 1}^n \alpha_i b_i\right)\right) \\
            &= f\left(\sum_{i = 1}^n \alpha_i g(b_i)\right) \\
            &= \sum_{i = 1}^n \alpha_i f(g(b_i)) \\
            &= \sum_{i = 1}^n \alpha_i b_i \\
            &= w \\
        \end{align*}
        \item thus, we have found a right inverse, as required
    \end{itemize}
\end{itemize}

\end{proof}

\section{The Rank-Nullity Theorem}

\subsection{Images and Kernels}

\begin{itemize}
    \item \textbf{What is the image of a homomorphism?}
    \begin{itemize}
        \item let $f$ be a homomorphism $f : V \to W$
        \item the subset $f(V) \subseteq W$ is the \textbf{image} of $f$
        \item we denote it $im(f) = f(V)$
        \item $im(f)$ is a \textbf{subspace} of $W$
    \end{itemize}
    \item \textbf{What is the kernel of a homomorphism?}
    \begin{itemize}
        \item the \textbf{preimage} of $\vec{0} \in W$
        \item in other words:
        \[
        ker(f) = \{\vec{v} \ | \ \vec{v} \in V, f(\vec{v}) = 0\}
        \]
        \item $ker(f)$ is a \textbf{subspace} of $V$
    \end{itemize}
\end{itemize}

\subsubsection{Examples}

\begin{itemize}
    \item if $f(a,b) = (0,0)$, then:
    \[
    ker(f) = \mathbb{R}^2 \qquad im(f) = \{(0,0)\}
    \]
    \item if $f(a,b) = (b-a, a-b)$, then:
    \[
    ker(f) = \{(a,a) \ | \ a \in \mathbb{R}\}
    \]
    \[
    im(f) = \{(c,-c) \ | \ c \in \mathbb{R}\}
    \]
    \item if $f(a,b) = (-b-a, a-b)$, then:
    \[
    ker(f) = \{(0,0) \}
    \]
    (since we require $-b-a = 0 = a - b \ \implies \ a = -a \ \implies \ a = 0$)
    \[
    im(f) = \mathbb{R}^2
    \]
    (since we obtain a system
    \[
    -b - a = x, x \in \mathbb{R}
    \]
    \[
    a - b = y, y \in \mathbb{R}
    \]
    which has unique solutions:
    \[
    2a = y-x \ \implies \ a = \frac{y-x}{2}
    \]
    \[
    b = \frac{y-x}{2} - y = \frac{-y-x}{2}
    \]
    )
\end{itemize}

\subsection{Lemma: Injectivity and the Kernel}

\textbox{A homomorphism is injective \textbf{if and only if} its kernel \textbf{only} contains $\vec{0}$. [Lemma 1.8.2]}

\begin{proof}

We prove in both directions.

\begin{itemize}
    \item $(\Longrightarrow)$: assume that $f$ is an injective homomorphism. Then, since $f(\vec{0}) = \vec{0}$, no other element of $V$ will be mapped to $\vec{0}$ by $f$, so:
    \[
    ker(f) = \{\vec{0}\}
    \]
    \item $(\Longleftarrow)$: assume that $ker(f) = \{\vec{0}\}$. Further, assume that $f(\vec{v}_1) = f(\vec{v}_2)$. Then:
    \begin{align*}
        &f(\vec{v}_1) - f(\vec{v}_2) = \vec{0} \\
        \implies &f(\vec{v}_1 - \vec{v}_2) = \vec{0} \ \text{\textit{(by linearity of f)}}\\
        \implies &\vec{v}_1 - \vec{v}_2 \in ker(f) \\
        \implies &\vec{v}_1 - \vec{v}_2 = 0 \\
        \implies &\vec{v}_1 = \vec{v}_2
    \end{align*}
    Hence, it follows that $f$ is injective.
\end{itemize}

\end{proof}

\subsection{Theorem: Rank-Nullity Theorem}

\begin{itemize}
    \item \textbf{What is the rank of a homomorphism?}
    \begin{itemize}
        \item the dimension of $im(f)$
    \end{itemize}
    \item \textbf{What is the nullity of a homomorphism?}
    \begin{itemize}
        \item the dimension of $ker(f)$
    \end{itemize}
\end{itemize}

\sep 

\textbox{Let $f: V \to W$ be a homomorphism. Then:
\[
dim V = dim(ker(f)) + dim(im(f))
\]
[Theorem 1.8.4]}

\begin{proof}

We begin by noticing that if $V$ is a finitely generated vector space, then:
\begin{itemize}
    \item since $ker(f)$ is a subspace of $V$, then $dim(ker(f)) \leq dim(V)$
    \item if $E$ is a generating set of $V$, then $f(E)$ is a generating set for $f(V) = im(f)$. Hence, $im(f)$ must also be finitely generated.
\end{itemize}

We first note that, if the rank or nullity of $f$ are infinite,we can immediately see that the $dim V = \infty$ (by the work above).

\bigskip

We now consider having a finite rank and nullity. Then, we can define the basis of $ker(f)$:
\[
A = \{v_1, \ldots, v_r\}
\]
and of $im(f)$:
\[
B = \{w_1, \ldots, w_s\}
\]
Our aim is to show that there exists some basis $E$ of $V$, such that:
\[
|E| = r + s
\]
To do this, define $\bar{w}_i \in V$, such that:
\[
f(\bar{w}_i) = w_i
\]
We know that such $\bar{w}_i$ exist, since $w_i \in im(f)$ (they are basis vectors of $im(f)$). We claim that:
\[
E = \{v_1, \ldots, v_r, \bar{w}_1, \ldots, \bar{w}_s\}
\]
Hence, we need to show that:
\begin{itemize}
    \item $\langle E \rangle = V$
    \begin{itemize}
        \item since $B$ is a basis for $f(V)$, we know that we can find $\alpha_i \in \F$ and $v \in V$ such that:
        \[
        f(v) = \sum_{i = 1}^s \alpha_i w_i
        \]
        \item now consider:
        \[
        f\left(v - \sum_{i = 1}^s \alpha_i \bar{w}_i\right)
        \]
        Applying linearity:
        \[
        f\left(v - \sum_{i = 1}^s \alpha_i \bar{w}_i\right) = f(v) - \sum_{i = 1}^s \alpha_i f(\bar{w}_i) = 0
        \]
        where we have used the fact that $f(\bar{w}_i) = w_i$.
        \item we thus know that:
        \[
        v - \sum_{i = 1}^s \alpha_i \bar{w}_i \in ker(f)
        \]
        \item then, since $A$ is a basis for $ker(f)$, we can write:
        \[
        v - \sum_{i = 1}^s \alpha_i \bar{w}_i = \sum_{i = 1}^r \beta_i v_i \ \implies \ v = \sum_{i = 1}^r \beta_i v_i + \sum_{i = 1}^s \alpha_i \bar{w}_i
        \]
        \item thus, we have shown that if $v \in V$, it can be generated by $E$
    \end{itemize}
    \item $E$ is linearly independent
    \begin{itemize}
        \item assume that we have $\alpha_i, \beta_i$ such that:
        \[
        \sum_{i = 1}^r \beta_i v_i + \sum_{i = 1}^s \alpha_i \bar{w}_i = 0
        \]
        \item applying $f$:
        \[
        f\left(\sum_{i = 1}^r \beta_i v_i + \sum_{i = 1}^s \alpha_i \bar{w}_i\right) = f(0) \ \implies \ \sum_{i = 1}^r \beta_i f(v_i) + \sum_{i = 1}^s \alpha_i w_i = 0
        \]
        \item since $v_i \in A, v_i \in ker(f)$, we have that:
        \[
        \sum_{i = 1}^r \beta_i f(v_i) = 0
        \]
        \item moreover, since $w_i$ are part of the basis $B$, they are linearly independent, so if $\sum_{i = 1}^s \alpha_i w_i = 0$, then $\alpha_i = 0$
        \item if each of the $\alpha_i$ are 0, then we have:
        \[
        \sum_{i = 1}^r \beta_i v_i  = 0
        \]
        \item again, the $v_i$ are part of $A$, so they are linearly independent, and so, $\beta_i = 0$
        \item thus, the elements in $E$ are linearly independent
    \end{itemize}
\end{itemize}

Thus, $E$ is a basis for $V$, and so:
\[
dim V = |E| = r + s
\]

\end{proof}

\subsubsection{Exercises (TODO)}

\begin{questions}

\question \textbf{Show that two subspaces $U,W$ of a vector space $V$ are complementary if and only if:
\begin{itemize}
    \item $V = U + W$
    \item $U \cap W = \{\vec{0}\}$
\end{itemize}}

Recall, $U,W$ are complementary if and only if the following bijection exists:
\[
\phi : (u,w) \to u+w, \qquad u\in U, w \in W
\]

We claim that:
\begin{itemize}
    \item $\phi$ is surjective \textbf{if and only if} $U + W = V$
    \item $\phi$ is injective \textbf{if and only if} $U \cap W = \vec{0}$
\end{itemize}

If $\phi$ is surjective, then $\forall v \in V, \exists u \in U,w \in W$ such that: 
\[
\phi(u,w) = u + w = v
\]
Hence, for any element in $v \in V$, we can find elements in $U,W$ which generate $v$, so:
\[
U + W = V
\]
Similarly, if $U + W = V$, then $\forall v \in V, \exists u \in U, w \in W$ such that:
\[
v = u+w
\]
But then, $u + w$ is nothing else but $\phi(u,w)$, so $\phi$ maps to every element in $V$, and so, $\phi$ is surjective.

Now assume that $\phi$ is injective. Furthermore, lets assume that $U \cap W \neq \{\vec{0}\}$. Then, we can find an element $a \in U \cap W$, with $a \neq \vec{0}$. But then:
\[
\phi(a,-a) = a - a = \vec{0}
\]
(this is well defined, since $a$ is in both $U$ and $W$). Since $\phi(0,0) = \vec{0}$, then clearly $\phi$ can't be injective, a contradiction. Hence, if $\phi$ is injective, then $U \cap W = \vec{0}$.

Now assume that $U \cap W = \{\vec{0}\}$. $\phi$ will be injective if $\phi(u,w) = \vec{0}$ is only possible when $u = w = \vec{0}$ (since we know that $\phi(0,0) = \vec{0})$. Consider $u, w \neq 0$ with $\phi(u,w) = 0$. Then:
\[
u+w = 0 \ \implies \ u = -w
\]
This means that $u \in W$ and $W \in U$. Hence, $u, w \in U \cap W$. But $U \cap W = \{\vec{0}\}$, so $u = w = \vec{0}$, as required.

Hence, $\phi$ is a bijection (and so $U,W$ are complementary) if and only if $U * W = V$ \textbf{and} $U \cap W = \{\vec{0}\}$

\question \textbf{Show that two subspaces $U,W$ of a vector space $V$ are complementary if and only if:
\begin{itemize}
    \item $V = U + W$
    \item $dim U + dim W \leq dim V$
\end{itemize}}

\question \textbf{Show that the kernel of a non-zero linear mapping $V \to F$ is a hyperplane, in the sense that together with another vector, the hyperplane and the vector generate $V$.}

\question \textbf{Let
\[
\phi : V \to V
\]
be an endomorphism of a finitely dimensional vector space $V$. Show that:
\[
ker(\phi \circ \phi) = ker(\phi)
\]
if and only if:
\[
V = ker(\phi) \oplus im(\phi)
\]}

We begin by noting that $V = ker(\phi) \oplus im(\phi)$ is equivalent to saying that $ker(\phi)$ and $im(\phi)$ are \textbf{complementary}, and so, by the previous exercise, it is true if and only if:
\begin{itemize}
    \item $V = U + W$
    \item $U \cap W = \{\vec{0}\}$
\end{itemize}

We proceed with the proof:

\begin{itemize}
    \item $(1 \Longrightarrow 2)$: assume that
    \[
    ker(\phi \circ \phi) = ker(\phi)
    \]
    \begin{itemize}
        \item define $U = ker(\phi)$ and $W = im(\phi)$
        \item pick $v \in U \cap W$
        \item since $v \in U$, we know that $\phi(v) = 0$
        \item since $v \in W$, we know that $\exists \bar{v} \in V$ such that:
        \[
        \phi(\bar{v}) = v
        \]
        \item now consider:
        \[
        \phi^2(\bar{v}) = \phi(\phi(\bar{v})) = \phi(v) = 0
        \]
        \item in other words, $\bar{v} \in ker(\phi \circ \phi)$
        \item but by assumption, $ker(\phi \circ \phi) = ker(\phi)$, so $\bar{v} \in ker(\phi)$
        \item hence, $\phi(\bar{v}) = 0$
        \item but since $\phi(\bar{v}) = v$ it follows that $v = 0$
        \item hence, any element $v \in U \cap W$ must be 0, so:
        \[
         U \cap W = \{0\}
        \]
        \item by the equivalence outlined at the start, we must then have $V = ker(\phi) \oplus im(\phi)$
    \end{itemize}
    \item $(2 \Longrightarrow 1)$: assume that
    \[
    V = ker(\phi) \oplus im(\phi)
    \]
    \begin{itemize}
        \item notice, if $v \in ker(\phi)$, then:
        \[
        \phi \circ \phi(v) = \phi(0) = 0
        \]
        so any element in $ker(\phi)$ is also in $ker(\phi \circ \phi)$, so:
        \[
        ker(\phi) \subseteq ker(\phi \circ \phi)
        \]
        \item pick $v \in ker(\phi \circ \phi)$. then:
        \[
        \phi^2(v) = \phi(\phi(v)) = 0
        \]
        In other words, $\phi(v) \in ker(\phi)$
        \item notice, $\phi(v) \in ker(\phi) = U$, but also $\phi(v) \in im(f) = W$, so we have that:
        \[
        \phi(v) \in U \cap W
        \]
        \item by assumption, we know that $V = ker(\phi) \oplus im(\phi)$ implies that $U \cap W = \{0\}$, so it follows that:
        \[
        \phi(v) = 0
        \]
        \item hence, if $v \in ker(\phi \circ \phi)$, then also $v \in ker(\phi)$, so:
        \[
        ker(\phi \circ \phi) \subseteq ker(\phi)
        \]
        \item in other words, if $V = ker(\phi) \oplus im(\phi)$, then:
        \[
        ker(\phi \circ \phi) = ker(\phi)
        \]
        as required
    \end{itemize}
\end{itemize}

What this is saying is that, if we have an idempotent endomorphism, we can decompose the vector space using the kernel and image of the endomorphism.

\question \textbf{An element $f$ is idempotent if $f^2 = f$. By the previous exercise, the idempotent endomorphisms of $V$ correspond uniquely to a decomposition of $V$ into a direct product of complementary subspaces. Show that:
\[
f \to (im(f), ker(f))
\]
leads to a bijection:
\[
\{f \ | \ f \in End(V), f^2 = f\} \to \{(I,K) \ | \ (I,K) \in \mathcal{P}(V)^2, I,K \subseteq V, I \oplus K = V\}
\]}

\end{questions}

\section{Linear Mappings and Matrices}

\subsection{Theorem: Assigning Matrices to Linear Mappings}

\textbox{Let $\F$ be a \textbf{field}, and let $m,n \in \mathbb{N}$.
\\
There exists a \textbf{bijection} between:
\begin{itemize}
    \item the space of homomorphisms $\F^m \to \F^n$
    \item the set of $n \times m$ matrices with entries in $\F$
\end{itemize}
via:
\[
M: Hom_\F(\F^m, \F^n) \to Mat(n \times m; \F)
\]
\[
M : f \to [f]
\]
We call $[f]$ the \textbf{representing matrix} of the mapping $f$.
\\
The columns of $[f]$ are given by applying $f$ to the standard basis vectors $\vec{e}_i, i \in [1,m]$ of $\F^m$:
\[
[f] := \left(f(\vec{e}_1) \ | \ \ldots \ | \ f(\vec{e}_m)\right)
\]
[Theorem 2.1.1]}

\begin{proof}

This uses $\eqref{l178}$, using $V = \F^m, W = \F^n$. We see that the homomorphism $f$ is determined by what it does to the basis elements of $V = \F^m$

\end{proof}

\subsubsection{Examples}

\begin{itemize}
    \item the \textbf{identity matrix} is defined by the identity homomorphism given by $id_{\F^m}(\vec{e}_i) = \vec{e}_i$:
    \[
    \mathbb{I} = [id_{\F^m}] = \begin{pmatrix}
    1 & 0 & \ldots & 0 \\
    0 & 1 & \ldots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \ldots & 1
    \end{pmatrix}
    \]
    Each column is just the elements of the standard basis $\vec{e}_i$.
    Conciseley, $\mathbb{I}_{ij} = \delta_{ij}$, the Kronecker Delta.
    \item if $m \geq n$ and $f$ is the homomorphism:
    \[
    f : (x_1, \ldots, x_m) \to (x_1, \ldots, x_n)
    \]
    (in other words, elements beyond $x_n$ are ``ignored), the corresponding matrix will be given by:
    \[
    A_{ij} = \begin{cases}
    \delta_{ij}, \qquad j \leq n \\
    0, \qquad j > n
    \end{cases}
    \]
    so:
    \[
    A = \begin{pmatrix}
    1 & 0 & \ldots & 0 & 0 & \ldots & 0 \\
    0 & 1 & \ldots & 0 & 0 & \ldots & 0\\
    \vdots & \vdots & \ddots & \vdots & \vdots & \ldots & \vdots \\
    0 & 0 & \ldots & 1 & 0 & \ldots & 0
    \end{pmatrix}
    \]
    (since once $i,j > n$, $f$ maps each $\vec{e}_i$ to the 0 vector)
    \item if $g : (x,y) \to (y,x)$ permutes coordinates in $\F^2$, the corresponding matrix is:
    \[
    [g] = \begin{pmatrix}
    0 & 1 \\
    1 & 0
    \end{pmatrix}
    \]
    More generally, we can define a \textbf{permutation matrix} by using a permutation $\pi \in S_n$, such that:
    \[
    P_\pi(\vec{e}_i) = \vec{e}_{\pi(i)}
    \]
    \pagebreak
    \item if $f$ is the reflection about the straight line making an agle $\alpha$ with the x-axis:
    \pic{matreflect.png}{0.7}
    Then:
    \[
    [f] = \begin{pmatrix}
    \cos(2\alpha) & \sin(2\alpha) \\
    \sin(2\alpha) & -\cos(2\alpha)
    \end{pmatrix}
    \]
    This follows from the fact that, if for example $\vec{e}_1 = (1,0)$, we can think of such a reflection as a rotation by $2\alpha$ radians about the origin. Then, the coordinates will just be the coordinates of the circle, after traversing $2\alpha$ rad. These can be derived, by using a right angle triangle, with unit hypotenuse, and angles $2\alpha, \frac{\pi}{2}, \frac{\pi}{2} - 2\alpha$ Hence:
    \[
    f(\vec{e}_1) = (\cos(2\alpha), \sin(2\alpha))
    \]
    For $\vec{e}_2$, we just rotate in the oppositve direction, and using an angle of $\pi - 2\alpha$:
    \[
    f(\vec{e}_2) = (\cos(\pi - 2\alpha), -\sin(\pi - 2\alpha)) = (\sin(2\alpha), -\cos(2\alpha))
    \]
\end{itemize}

\subsection{Theorem: Composition of Linear Mappings and Products of Matrices)}

\begin{itemize}
    \item \textbf{How do we define multiplication of matrices?}
    \begin{itemize}
        \item 2 matrices $A,B$ can be multiplied to give a product $A \circ B = AB$ if:
        \[
        A \in Mat(n \times m; \F)
        \]
        \[
        B \in Mat(m\times l; \F)
        \]
        \item their product $AB \in Mat(n \times l; \F)$ is given by:
        \[
        AB_{ik} = \sum_{j = 1}^m A_{ij}B_{jl}, \qquad i \in [1,n], k \in [1,l]
        \]
        \item in other words $A_{ik}$ is given by taking the \textbf{dot product} of the $i$th row of $A$, and the $k$th column of $B$ 
    \end{itemize}
    \item \textbf{Is matrix multiplication a mapping?}
    \begin{itemize}
        \item yes, of the form:
        \[
        Mat(n \times m; \F) \times Mat(m\times l; \F) \to  Mat(n \times l; \F)
        \]
        via:
        \[
        (A,B) \to AB
        \]
    \end{itemize}
\end{itemize}

\sep 
\label{t218}
\textbox{Consider the homomorphisms:
\[
g : \F^l \to \F^m
\]
\[
f : \F^m \to \F^n
\]
Then, the \textbf{representing matrix} of $f \circ g$ is the product of the representing matrices of $f$ and $g$. In other words:
\[
[f \circ g] = [f] \circ [g]
\]
[Theorem 2.1.8]}

\begin{proof}

Lets define the matrices, and the bases of the spaces:
\[
A = [f], \qquad \F^m = \langle \{\vec{a}_i \ | \ i \in [1,m]\}\rangle
\]
\[
B = [g], \qquad \F^l = \langle \{\vec{b}_j \ | \ j \in [1,l]\}\rangle
\]
\[
C = [f]\circ[g], \qquad \F^n = \{\vec{c}_k \ | \ k \in [1,n]\}
\]
Then, by how we define the bijection from homomorphisms to vectors, we know that:
\[
f(\vec{a}_i) = A_{*i} = \sum_{k = 1}^n A_{ki}\vec{c}_k  
\]
\[
g(\vec{b}_j) = B_{*j} = \sum_{i = 1}^m B_{ij}\vec{a}_i  
\]
What this is saying is that, for example, for the $i$th column of $[f]$ (denoted $A_{*i}$), we are taking an element from the basis of $\F^m$, and mapping it to an element of $\F^n$, by using a linear combination of basis vectors of $\F^n$. The ``coordinates" of the element in $\F^n$ to which we map are precisely the coefficients of this linear combination, which is given by the matrix entries $A_{ki}$. 

\bigskip

Using this, we can write:
\begin{align*}
    (f \circ g)(\vec{\vec{b}_j}) &= f\left(\sum_{i = 1}^m B_{ij}\vec{a}_i  \right) \\
    &= \sum_{i = 1}^m B_{ij}f(\vec{a}_i) \\
    &= \sum_{i = 1}^m B_{ij}\sum_{k = 1}^n A_{ki}\vec{c}_k   \\
    &= \sum_{i = 1}^m \sum_{k = 1}^n (A_{ki}B_{ij})\vec{c}_k   \\
    &=  \sum_{k = 1}^n \left(\sum_{i = 1}^mA_{ki}B_{ij}\right)\vec{c}_k   \qquad \text{\textit{(we can switch the sums, since they are finite)}}\\
    &= \sum_{k = 1}^n C_{kj}\vec{c}_k \qquad \text{\textit{(by definition of matrix product, $C_{kj} = \sum_{i = 1}^mA_{ki}B_{ij}$)}} \\
\end{align*}

Notice, this is just giving the $j$th column of the matrix $C$, defined by $[f] \circ [g]$, so it follows that:
\[
[f \circ g] = [f] \circ [g]
\]

\end{proof}

\subsection{Proposition: Calculating With Matrices}

\textbox{Define the following matrices:
\begin{itemize}
    \item $A,A' \in Mat(n \times m; \F)$
    \item $B,B' \in Mat(m \times l; \F)$
    \item $C \in Mat(l \times k; \F)$
    \item $I_m$, the $m \times m$ identity matrix
\end{itemize}
Then, the following hold for matrix multiplication:
\begin{enumerate}
    \item 
    \[
    (A + A')B = AB + A'B
    \]
    \item
    \[
    A(B + B') = AB + AB'
    \]
    \item 
    \[
    I_mB = B
    \]
    \item 
    \[
    AI_m = A
    \]
    \item
    \[
    (AB)C = A(BC)
    \]
\end{enumerate}
[Proposition 2.1.9]}

\begin{proof}

Whilst these can be proven from first principles (i.e using all the summation business), it is more elegant to use the bijection between homomorphisms and matrices, alongside the fact that $[f \circ g] = [f] \circ [g]$, and the distributive and associative property of functions.

\bigskip

Let:
\begin{itemize}
    \item $[f] = A$
    \item $[f'] = A'$
    \item $[g] = B$
    \item $[g'] = B'$
    \item $[h] = C$
    \item $[id_{\F^m}] = I_m$
\end{itemize}

\begin{enumerate}
    \item $[(f + f')] \circ [g] = [(f + f') \circ g] = [f \circ g + f' \circ g] = [f \circ g] + [f' \circ g]$
    \item use similar logic as above
    \item $[id_{\F^m}] \circ [g] = [id_{\F^m} \circ g] = [g]$
    \item use similar logic as above
    \item $([f] \circ [g]) \circ [h] = [f \circ g] \circ [h] = [(f \circ g) \circ h] = [f \circ (g \circ h)] = f \circ [g \circ h] = f \circ ([g] \circ [h]) $
\end{enumerate}

\end{proof}

\subsection{Remark: Assigning Linear Mappings to Matrices}

\textbox{Above we discussed how the mapping:
\[
M : Hom_\F(\F^m, \F^n) \to Mat(n \times m; \F)
\]
produces a \textbf{representing matrix} from a homomorphism.
\\
We can easily define the inverse transformation, by thinking of applying a matrix $A \in Mat(n \times m; \F)$ to an element of $\F^m$, and producing an element of $\F^n$:
\[
(A \circ) : \F^m \to \F^n
\]
Indeed, this is our old intuition of ``multiplying a vector by a matrix":
\[
A\vec{x} = \vec{b}, \vec{x} \in \F^m, \vec{b} \in \F^n
\]
(here we are taking liberty, since technically for this to be applicable we should use $Mat(m \times 1; \F)$ instead of $\F^m$, and $Mat(n \times 1; \F$ instead of $\F^n$).
\\
This then allows us to define the inverse of $M$:
\[
M^{-1} : Mat(n \times m; \F) \to Hom_\F(\F^m, \F^n) 
\]
via:
\[
A \to (A \circ)
\]
In other words, to each matrix, we associate a linear map.
[Remark 2.1.10]}

\subsubsection{Exercises (TODO)}

\begin{questions}

\question \textbf{Let $f : \mathbb{R}^2 \to \mathbb{R}^2$ be the reflection:
\[
(x,y) \to (x,-y)
\]
Show that:
\[
\{g \ | \ g \in Hom_\mathbb{R}(\mathbb{R}^2, \mathbb{R}^2), f \circ g = g \circ f\}
\]
is a subspace of $Hom_\mathbb{R}(\mathbb{R}^2, \mathbb{R}^2)$, and give a basis of this subspace.}

\question \textbf{Define the \textit{transpose} of a matrix via:
\[
(A^T)_{ij} = A_{ji}
\]
Show that:
\begin{itemize}
    \item $(A^T)^T = A$
    \item $(AB)^T = B^TA^T$
\end{itemize}
}

\end{questions}

\section{Properties of Matrices}

\subsection{Invertibility of a Matrix}

\begin{itemize}
    \item \textbf{When is a matrix invertible?}
    \begin{itemize}
        \item when it has \textbf{both} a \textbf{left} and \textbf{right} inverse
        \item in other words, $A$ is invertible \textbf{if and only if} $\exists B,C$ such that:
        \[
        AB = CA = \mathbb{I}
        \]
        \item notice, a matrix may have a left \textbf{or} a right inverse, but said matrix won't be invertible. For example:
        \[
        \begin{pmatrix}
        1 & 0
        \end{pmatrix}
        \begin{pmatrix}
        1 \\
        0
        \end{pmatrix}
        = \mathbb{I}
        \]
        but $\begin{pmatrix}
        1 & 0
        \end{pmatrix}$ doesn't have a left inverse, so it isn't invertible
    \end{itemize}
    \item \textbf{How does the isomorphism defining the matrix define its invertibility?}
    \begin{itemize}
        \item consider a matrix $A$, defined as a homomorphism:
        \[
        a : \F^n \to \F^m
        \]
        \item by \eqref{t218}, we can ``translate" the requirements for matrix invertibility to be in terms of function composition
        \item hence, $A$ is invertible if and only if we can find $b,c$ such that:
        \[
        a \circ b = id_{\F^k}, \qquad b : \F^k \to F^n
        \]
        \[
        c \circ a = id_{\F^n}, \qquad c : \F^m\to F^l
        \]
        \item notice, the identity function is an isomorphism, so $a \circ b$ and $c \circ a$ must be isomorphisms. Hence:  
        \begin{itemize}
            \item $\F^m$ is isomorphic to $\F^k$, and since vector spaces are characterised by their dimension [\eqref{t177}], we must have $m = k$
            \item $\F^n$ is isomorphic to $\F^l$, and since vector spaces are characterised by their dimension [\eqref{t177}], we must have $n = l$
        \end{itemize}
        \item notice that since the compositions are isomorphism, we must then again have that $n = m$, so in particular, if $A$ is invertible, $n = m$ - A must be a square matrix
    \end{itemize}
    \item \textbf{Are all matrices invertible?}
    \begin{itemize}
        \item only \textbf{square} matrices are invertible
        \item this is summarised by the following set of equivalences:
        \begin{enumerate}
            \item There exists a square matrix $B$ such that:
            \[
            BA = \mathbb{I}
            \]
            \item There exists a square matrix $C$ such that:
            \[
            AC = \mathbb{I}
            \]
            \item The square matrix $A$ is invertible
        \end{enumerate}
        \begin{proof}
        To show equivalence, we show that 3 implies 1 and 2, and that 1 and 2 each imply 3.
        \begin{itemize}
            \item $\left(\circled{3} \implies \circled{1}, \circled{2}\right)$: this is from the definition of matrix invertibility
            \item $\left(\circled{1} \implies \circled{3}\right)$: lets assume that $A$ has a left inverse. Then, it must be the case that $a$ (where $a : \F^n \to \F^n$) has a left inverse under function composition, so $a$ is injective. Then, $ker(a) = \{0\}$, so by rank nullity theorem:
            \[
            dim(\F^n) = dim(im(a))
            \]
            Since $im(a)$ is a vector subspace of $\F^n$, it follows by Remark 1.6.9 that $im(a) = \F^n$, so in particular $a$ is surjective. Hence, $a$ must be an isomorphism, so $A$ must be invertible.
            \item $\left(\circled{2} \implies \circled{3}\right)$: lets assume that $A$ has a right inverse. Then, it must be the case that $a$ (where $a : \F^n \to \F^n$) has a right inverse under function composition, so $a$ is surjective. By rank nullity theorem:
            \[
            dim(\F^n) = dim(im(a)) + dim(ker(a))
            \]
            Since $dim(im(a)) = dim(\F^n)$, we must have that $dim(ker(a)) = 0$, so it follows that $a$ is also injective.
            Hence, $a$ must be an isomorphism, so $A$ must be invertible.
        \end{itemize}
        \end{proof}
    \end{itemize}
    \item \textbf{How do we denote the inverse matrix?}
    \begin{itemize}
        \item if $a^{-1}$ is the inverse of the mapping $a$ defining a matrix $A$, we can denote the inverse of $A$ via:
        \[
        [a^{-1}] = A^{-1}
        \]
    \end{itemize}
\end{itemize}

\subsubsection{Exercises}

\begin{questions}

\question \textbf{Show that the \textit{general linear group of $2 \times 2$}, $GL(2;\F_2)$, and $S_3$ are isomorphic.}

\question \textbf{Is the group $GL(1;\F_p)$ abelian? What is its order?}

\question \textbf{What is the center of $GL(n;\F)$? What is the order of the center of $GL(n;\F_p)$?}

\end{questions}

\subsection{Elementary Row Operations as Matrices}

\begin{itemize}
    \item \textbf{What is the basis matrix?}
    \begin{itemize}
        \item a matrix $E_{ij}$, such that:
        \[
        (E_{ij})_{lm} = \begin{cases}
        1, \qquad l = i, m = j \\
        0, \qquad \text{\textit{otherwise}}
        \end{cases}
        \]
    \end{itemize}
    \item \textbf{How can row addition be represented as a matrix?}
    \begin{itemize}
        \item we use the matrix:
        \[
        \mathbb{I} + \lambda E_{ij}
        \]
        to add $\lambda$ times the $j$th row to the $i$th row
        \item as an example, if we want to add the 2 times the 3rd row to the 1st column of:
        \[\begin{pmatrix}
        1 & 2 & 3 \\
        1 & 1 & 1 \\
        3 & 2 & 1
        \end{pmatrix}
        \]
        we apply the above matrix:
        \[
        \begin{pmatrix}
        1 & 0 & 2 \\
        0 & 1 & 0 \\
        0 & 0 & 1
        \end{pmatrix}
        \begin{pmatrix}
        1 & 2 & 3 \\
        1 & 1 & 1 \\
        3 & 2 & 1
        \end{pmatrix}
        = \begin{pmatrix}
        1 + 2(3) & 2 + 2(2) & 3 + 2(1) \\
        1 & 1 & 1 \\
        3 & 2 & 1
        \end{pmatrix}
        \]
        \item from this we see that $E_{ij}$ does nothing but ``pick up" the $j$th row of the matrix to which it is applied on, and puts it in the $i$th row of the resulting matrix
        \item notice, $\mathbb{I} + \lambda E_{ij}$ is \textbf{invertible} (since $(\mathbb{I} + \lambda E_{ij})(\mathbb{I} - \lambda E_{ij}) = \mathbb{I}$) so applying it is reversible, thus preserving the solution
    \end{itemize}
    \item \textbf{How can row swap be represented as a matrix?}
    \begin{itemize}
        \item define the matrix $P_{ij}$ which swaps row $i$ with row $j$
        \item $P_{ij}$ will just be the identity matrix, with its own $i$th and $j$th rows swapped
        \item this can be thought as a homomorphism $\F^m \to \F^m$
        \item again, $P_{ij}P_{ij} = \mathbb{I}$, so the operation is reversible
    \end{itemize}
\end{itemize}

\subsection{Theorem: Elementary Matrices as Building Blocks}

\begin{itemize}
    \item \textbf{What is an elementary matrix?}
    \begin{itemize}
        \item a (square) matrix differing from $\mathbb{I}$ in \textbf{at most} one entry
    \end{itemize}
    \item \textbf{When are elementary matrices invertible?}
    \begin{itemize}
        \item so long as the change to $I$ doesn't change a 1 by a 0, the elementary matrix is invertible
    \end{itemize}
\end{itemize}

\sep 

\textbox{Every \textbf{square} matrix with entries in a field can be written as a \textbf{product} of \textbf{elementary
matrices}.
[Theorem 2.2.3]}

(This \href{https://youtu.be/pMp8RfWtPwQ}{here} is a nice work through; in this proof, they choose to apply right multiplications, when, as far as I am concerned, applying left EROs should be sufficient)

\begin{proof}

Notice, we can represent any permutation matrix as a \textbf{product} of elementary matrices, namely:
\[
P_{ij} = diag(1,\ldots,1,-1,1,\ldots,1)(\mathbb{I} + E_{ij})(\mathbb{I} - E_{ji})(\mathbb{I} + E_{ij})
\]
where the $-1$ in $diag(1,\ldots,1,-1,1,\ldots,1)$ is at the $j$th diagonal entry.

Instead of getting all theoretical, lets use an example, with a 3 \times 3 matrix. For example, if we want to swap the first and second rows:
\begin{align*}
    &\begin{pmatrix}
        1 & 0 & 0 \\
        0 & -1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \begin{pmatrix}
        1 & 1 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \begin{pmatrix}
        1 & 0 & 0 \\
        -1 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \begin{pmatrix}
        1 & 1 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \\
    =&
    \begin{pmatrix}
        1 & 0 & 0 \\
        0 & -1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \begin{pmatrix}
        1 & 1 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \begin{pmatrix}
        1 & 1 & 0 \\
        -1 & 0 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \\
    =&
    \begin{pmatrix}
        1 & 0 & 0 \\
        0 & -1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \begin{pmatrix}
        0 & 1 & 0 \\
        -1 & 0 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \\
    =&
    \begin{pmatrix}
        0 & 1 & 0 \\
        1 & 0 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \\
\end{align*}
If we then apply this to our matrix above:
\[
\begin{pmatrix}
        0 & 1 & 0 \\
        1 & 0 & 0 \\
        0 & 0 & 1
    \end{pmatrix}
    \begin{pmatrix}
        1 & 2 & 3 \\
        1 & 1 & 1 \\
        3 & 2 & 1
        \end{pmatrix}
    =
    \begin{pmatrix}
        1 & 1 & 1 \\
        1 & 2 & 3 \\
        3 & 2 & 1
        \end{pmatrix}
\]

The second thing to notice is that if an elementary matrix is invertible, then its inverse will also be an elementary matrix. We have already seen that for example $(\mathbb{I} + \lambda E_{ij})(\mathbb{I} - \lambda E_{ij}) = \mathbb{I}$. Moreover, a product of invertible matrices is also invertible (if $A,B$ are invrtible, the inverse of $AB$ is $B^{-1}A^{-1}$. Notice, any elementary matrix can be constructed by chaining row swaps and row additions. Row swaps are invertible (since they are a product of invertible matrices). Hence, any elementary matrix must be invertible.

\bigskip

With all this in mind, we can give the proof. Take an arbitrary matrix $A$. Then:
\begin{enumerate}
    \item We can find invertible, elementary matrices $S_1, S_2, \ldots, S_t$, such that:
    \[
    S_t\ldots S_1 A
    \]
    is in row echelon form (this is just Gaussian elimination through EROs)
    \item We can then reduce the columns, by applying a set of invertible, elementary matrices $T_1, \ldots, T_s$ to the \textbf{right} of the resulting matrix:
    \[
    S_t\ldots S_1 AT_1\ldots T_s
    \]
    \item by reducing rows and columns, we can obtain a diagonal matrix by simply using elementary matrices:
    \[
    D = diag(1,\ldots,1,0,\ldots,0)
    \]
    \item $D$ can be expressed using elementary matrices (apparently non-invertible) as well, so overall:
    \[
    S_t\ldots S_1 AT_1\ldots T_s = D_1\ldots D_k
    \]
    Hence, we can express $A$ as a product of elementary matrices:
    \[
    A = S_1^{-1}\ldots S_t^{-1}D_1\ldots D_kT_{s}^{-1}\ldots T_{1}^{-1}
    \]
    
\end{enumerate}

\end{proof}

\subsection{The Smith Normal Form}

\begin{itemize}
    \item \textbf{When is a matrix in Smith normal Form?}
    \begin{itemize}
        \item a matrix is in \textbf{Smith Normal Form} if all the non-zero entries are along the main diagonal, and the non-zero entries are consecutive 1s
        \[
        \begin{pmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0
        \end{pmatrix}
        \]
        \item alternatively, we can think of SNF as being an identity matrix, padded with 0s
    \end{itemize}
\end{itemize}

\subsection{Theorem: Transforming a Matrix into SNF}

\textbox{Given a matrix $A \in Mat(n \times m; \F$, there exist \textbf{invertible} matrices $P,Q$ such that $PAQ$ is in \textbf{Smith Normal Form}. [Theorem 2.2.5]}

\begin{proof}

We can find invertible elementary matrices $S_1, \ldots, S_t$ such that \[
S_1\ldots S_tA
\]
is in row echelon form (these matrices perform eleemntary row operations to achieve this)

\bigskip

Once in row echeleon form, we can apply more matrices $T_1, \ldots, T_s$, such that:
\[
S_1\ldots S_tAT_1\ldots T_s
\]
is brought to Smith Normal Form.

\bigskip

We can then just define:
\[
P = S_1\ldots S_t
\]
\[
Q = T_1\ldots T_s
\]

\end{proof}

\subsection{Matrix Rank}

\begin{itemize}
    \item \textbf{What is the column rank of a matrix?}
    \begin{itemize}
        \item the dimension of the space generated by the columns of a matrix
    \end{itemize}
    \item \textbf{What is the row rank of a matrix?}
    \begin{itemize}
        \item the dimension of the space generated by the rows of a matrix
    \end{itemize}
    \item \textbf{When does a matrix have full rank?}
    \begin{itemize}
        \item when both the column and row ranks are equal, and they are equal to the smallest number, out of the number of rows or columns
    \end{itemize}
\end{itemize}

\subsection{Theorem: Column and Row Rank}

\textbox{The \textbf{column rank} and the \textbf{row rank} of a matrix are \textbf{equal}.
[Theorem 2.2.8]}

\begin{proof}

(Own proof since the notes are absolute crap)

Notice, applying elementary row operations won't change the column rank of a matrix (and if we transpose it, EROs won't change the row rank). This is because elementary row operations are linear combinations of the columns/row vectors, so this won't change the number of linear independent vectors which are part of the matrix rows/columns. In particular, this means that any matrix $A$ has the same column and row rank as its corresponding SNF matrix. But a matrix in SNF has the same row and column ranks (since this is given by the number of LiD vectors, and these are given by the number of 1s present along the rows/columns, and this number is the same).
\end{proof}

\subsubsection{Exercises (TODO)}

\begin{questions}

\question \textbf{Find a $3 \times 3$ matrix with all entries non-zero, but with rank 2.}

\end{questions}

\subsection{Inverting Matrices}

\begin{itemize}
    \item \textbf{How can you invert a matrix?}
    \begin{itemize}
        \item the idea is to write an augmented matrix of the form:
        \[
        \begin{amatrix}{1}
        A & \mathbb{I}
        \end{amatrix}
        \]
        \item then, apply EROs, such that we obtain:
        \[
        \begin{amatrix}{1}
        \mathbb{I} & (S_1S_2\ldots S_t)\mathbb{I}
        \end{amatrix}
        \]
        \item then, the inverse will be:
        \[
        A^{-1} = S_1S_2\ldots S_t
        \]
    \end{itemize}
\end{itemize}

\subsubsection{Exercises (TODO)}

\begin{questions}

\question \textbf{Show that:
\[
rank(A + B) \leq rank(A) + rank(B)
\]}

\question \textbf{Show that:
\[
rank(AB) \leq min\{rank(A), rank(B)\}
\]}

\question \textbf{Let $r = rank(B)$, where $B$ is such that:
\[
B \in Mat(n \times r; \F)
\]
Show that there exists $E \in Mat(r \times n;  \F$ such that:
\[
EB = 1_r
\]
Similarly let $C \in Mat(r \times n; \F)$ where $rank(C) = r$. Then there exists $H \in Mat(n \times r; \F)$ such that $CH = 1_r$.}

\question \textbf{Let $A \in Mat(m \times n; \F)$ have rank $r$. Show that $A = BC$, where:
\begin{itemize}
    \item $B \in Mat(m \times r; \F)$
    \item $C \in Mat(r \times n; \F)$
\end{itemize}
and $rank(B) = rank(C) = r$. 
\begin{enumerate}
    \item Is this decomposition unique?
    \item Hence, or otherwise, show that $A$ can be decomposed as a sume of $r$ rank 1 matrices.
\end{enumerate}}


\end{questions}

\end{document}