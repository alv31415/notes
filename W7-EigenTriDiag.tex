\documentclass{exam}
\usepackage[utf8]{inputenc}

\usepackage{mynotes}

\title{Honours Algebra - Week 7 - Eigenvalues, Eigenvectors and Triangularisations}
\author{Antonio León Villares}
\date{February 2022}

\begin{document}

\maketitle

\tableofcontents

\pagebreak

\textit{Based on the notes by Iain Gordon, Sections 4.5 - 4.7}

\section{Eigenvalues and Eigenvectors}

\subsection{Definition of Eigenstuffs}

\begin{itemize}
    \item \textbf{What is an eigenvalue?}
    \begin{itemize}
        \item let $V$ be an $F$-vector space, and define an endomorphism:
        \[
        f : V \to V
        \]
        \item $\lambda \in F$ is an \textbf{eigenvalue} of $f$, if:
        \[
        \exists \vec{v} \in V \ : \ f(\vec{v}) = \lambda \vec{v}
        \]
    \end{itemize}
    \item \textbf{What is an eigenvector?}
        \begin{itemize}
            \item any vector $\vec{v} \in V$ such that $f(\vec{v}) = \lambda \vec{v}$
            \item $\vec{v}$ is the \textbf{eigenvector of $f$ with eigenvalue $\lambda$}
            \item \textbf{eigenvectors} are not unique; for example, if $F = \mathbb{R}$, and $\vec{v}$ is an eigenvector, then $2\vec{v}$ is also an eigenvector of the same eigenvalue
        \end{itemize}
        \item \textbf{What is an eigenspace?}
        \begin{itemize}
            \item the set of all \textbf{eigenvectors} of endomorphism $f$ with \textbf{eigenvalue} $\lambda \in F$:
            \[
            E(\lambda, f) = \{\vec{v} \ | \ \vec{v} \in V, f(\vec{v}) = \lambda \vec{v}\}
            \]
        \end{itemize}
\end{itemize}

\subsubsection{Examples}

\begin{itemize}
    \item the set of all \textbf{fixed points} of $f$ (i.e all $\vec{x}$ with $f(\vec{x}) = \vec{x}$) is the \textbf{eigenspace} corresponding to $\lambda = 1$
    \item the set of non-zero elements in $ker(f)$ is precisely the eigenspace of non-zero eigenvectors with $\lambda = 0$
    \item if $f$ is the endomorphism rotating the plane by 90º, this only has eigenvalues in $\mathbb{C}$
    \item if $f$ is the endomorphism representing a reflection on a line, the associated eigenvalue is 1, and the eigenvectors are all the eigenvectors which lie on the line (so the line is the eigenspace)
    \item if $f$ is the endomorphism representing a 180º rotation, this will have eigenvalue $-1$ (since this rotation maps a point to a point diametrically opposite)
    \item real polynomial differentiation only has $\lambda = 0$, with eigenvectors as the non-zero constant polynomials:
    \[
    p \in \mathbb{R}[x], deg(P) = 0 \ \implies \ D(p) = 0 = 0p
    \]
    For the rest of the chapter, to avoid confusion with matrices, polynomials will be denoted using $x$ (instead of $X$)
\end{itemize}

\subsubsection{Exercises (TODO)}

\begin{questions}
    \question \textbf{Let $f : V \to V$ be an endomorphism of an $F$-vector space $V$. Show that $E(\lambda, f)$ is a vector subspace of $V$, for any $\lambda \in F$}
\end{questions}

\subsection{Theorem: Eigenvalues and Characteristic Polynomials}

\begin{itemize}
    \item \textbf{What is the characteristic polynomial?}
    \begin{itemize}
        \item consider a \textbf{commutative ring} $R$, and let $A \in Mat(n;R)$
        \item the \textbf{characteristic polynomial of the matrix $A$ is the polynomial:}
        \[
        \X_A(x) := det(xI_n - A)
        \]
        \item for example,
        \[
        A = \begin{pmatrix}
        2 & 3 \\
        -6 & 1
        \end{pmatrix}
        \ \implies \
        xI_n - A = \begin{pmatrix}
        2-x & 3 \\
        -6 & 1-x
        \end{pmatrix}
        \ \implies \
        \X_A(x) = (2-x)(1 - x) + 18 = x^2 - 3x + 20
        \]
    \end{itemize}
\end{itemize}

\sep 

\textbox{
Let $F$ be a \textbf{field}, and let $A \in Mat(N;F)$.
\\
The \textbf{eigenvalues} of the \textbf{linear mapping}:
\[
A : F^n \to F^n
\]
are precisely the \textbf{roots} of $\X_A$. [Theorem 4.5.8]
}

\begin{proof}

For any $\lambda \in F$, $\lambda$ is an eigenvalue of $A$ if and only if:
\begin{align*}
    &\exists \vec{v} \neq 0 \ : \ A\vec{v} = \lambda \vec{v} \\
    \iff &\exists \vec{v} \neq 0 \ : \ (\lambda I_n - A)\vec{v} = 0 \\
    \iff & ker(\lambda I_n - A) \neq 0\\
\end{align*}
If the kernel is non-zero, then notice that we have a non-zero $\vec{v}$:
\[
(\lambda I_n - A)\vec{v} = \vec{0}
\]
If $(\lambda I_n - A)^{-1}$ existed, then:
\[
\vec{v} = (\lambda I_n - A)^{-1}\vec{0} = \vec{0}
\]
which is a contradiction. Thus, $(\lambda I_n - A)$ can't be invertible. In other words, $\lambda$ is an eigenvalue \textbf{if and only if}:
\[
det(\lambda I_n - A) = 0 \ \implies \ \X_A(\lambda) = 0
\]

\end{proof}

\subsubsection{Exercises (TODO)}

\begin{questions}

\question \textbf{Let $F$ be a field, and $A \in Mat(n;F)$. Show that:
\[
\X_A(x) = x^n - tr(A)x^{n-1} + (-1)^n det(A) + \sum_{i = 2}^{n-1} a_ix^{n - i}
\]}

\end{questions}

\subsection{Remark: Defining the Characteristic Polynomial of Endomorphisms}

\textbox{
Consider an endomorphism $f : V \to V$. If we have ordered bases:
\[
\mathcal{A} = (\vec{v}_1, \ldots, \vec{v}_n) \qquad \mathcal{B} = (\vec{w}_1, \ldots, \vec{w}_n)
\]
then we could define matrices $A,B \in Mat(n;R)$:
\[
A = (a_{ij}) = \cript{f}{\mathcal{A}}{\mathcal{A}} \qquad B = (b_{ij}) = \cript{f}{\mathcal{B}}{\mathcal{B}}
\]
where the $j$th columns of the matrices satisfied:
\[
f(\vec{v}_j) = \sum_{i = 1}^n a_{ij}\vec{v}_i \qquad f(\vec{w}_j) = \sum_{i = 1}^n b_{ij}\vec{w}_i
\]
The \textbf{change of basis matrix}, $P \in GL(n;R)$:
\[
P = (p_{ij}) = \cript{id_V}{\mathcal{A}}{\mathcal{B}} \qquad \vec{w}_j = \sum_{i = 1}^n p_{ij}\vec{v}_i
\]
allowed us to define the \textbf{trace} of an endomorphism, independent of a \textbf{basis}, since it allowed us to see $A,B$ as \textbf{conjugate matrices}:
\[
B = P^{-1}AP
\]
and the trace of conjugate matrices is the same.
\\
In a similar vein, the \textbf{characteristic polynomial} of \textbf{conjugate matrices} is the same, so we can define the \textbf{characteristic polynomial of an endomorphism}, by using its representing matrix, with respect to \textbf{any} basis.
\\
To see this:
\[
\X_B = det(xI_n - B) = det(xI_n - P^{-1}AP)
\]
Notice that:
\[
det(P^{-1}(xI_n - A)P) = det(xP^{-1}I_nP - P^{-1}AP) = det(xI_n P^{-1}AP)
\]
So:
\[
\X_B = det(P^{-1})det(xI_n - A)det(P) = det(xI_n - A) = \X_A
\]
Thus, the \textbf{eigenvalues} of $f$ are the roots of $\X_f$, the \textbf{characteristic polynomial of $f$}.
[Remark 4.5.9]
}

\subsubsection{Exercises (TODO)}

\begin{questions}

\question \textbf{Show that very endomorphism of an odd dimensional real vector space has a real eigenvalue. Show furthermore that if the determinant of the endomorphism is a positive real number, then the endomorphism even has a positive real eigenvalue.}

\end{questions}

\subsection{Remark: Existence of Upper Triangular Representing Matrix}

\textit{The following remark might look weird here, but it is useful in the next section, when triangularisation of matrices is discussed.}

\textbox{
Consider the endomorphism of an $n$:-dimensional $F$-vector space:
\[
f : V \to V
\]
Let $W$ be a vector \textbf{subspace} of $V$, with $f(W) \subseteq W$. Then, define the following endomorphisms:
\[
g : W \to W \qquad \vec{w} \to f(\vec{w})
\]
\[
h : V/W \to V/W \qquad W + \vec{v} \to W + f(\vec{v})
\]
We can construct a basis for $V$ using a basis for $W$:
\[
\mathcal{A} = (\vec{w}_1, \ldots, \vec{w}_m)
\]
\[
\mathcal{B} = (\vec{w}_1, \ldots, \vec{w}_m, \vec{v}_{m+1}, \ldots, \vec{v}_n)
\]
Moreover, the basis of $V/W$ can be constructed by applying the \textbf{cononical map} $can : V \to V/W$ to the elements $\vec{v}_j$ of the basis $\mathcal{B}$:
\[
\mathcal{C} = (can(\vec{v}_{m+1}, \ldots, can(\vec{v}_n))
\]
Now, turns out that we can write:
\[
\cript{f}{\mathcalB}{\mathcal{B}} = \begin{pmatrix}
\cript{g}{\mathcal{A}}{\mathcal{A}} & \cript{e}{\mathcal{A}}{\mathcal{C}} \\
0 & \cript{h}{\mathcal{C}}{\mathcal{C}}
\end{pmatrix}
=
\begin{pmatrix}
(a_{ij}) & (c_{ik}) \\
0 & (b_{jk})
\end{pmatrix}
\]
where the components $a_{ij}$ of $\cript{g}{\mathcal{A}}{\mathcal{A}}$ satisfy:
\[
f(\vec{w}_j) = \sum_{i = 1}^m a_{ij}\vec{w}_i
\]
and we have coefficients $b_{jk}, c_{ik}$ satisfying:
\[
f(\vec{v}_k) = \sum_{i = 1}^m c_{ik}\vec{w}_i + \sum_{j = m+1}^n b_{jk}\vec{v}_j
\]
Moreover:
\[
e : V/W \to W \qquad W + \vec{v}_k \to \sum_{i = 1}^m c_{ik}\vec{w}_i
\]
}



\end{document}